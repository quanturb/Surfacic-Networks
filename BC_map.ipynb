{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness centrality calculation for real data\n",
    "\n",
    "Surfacic Networks\n",
    "Barthelemy et al., PNAS Nexus 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%conda install -c conda-forge pandas\n",
    "#conda activate ox\n",
    "# generate x y random in disk(0,1) and z=1-x**2-y**2\n",
    "# generate then rgg\n",
    "\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import seaborn as sns\n",
    "from colorama import Fore\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "mpl.rc('figure', max_open_warning = 0)\n",
    "\n",
    "import itertools\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from haversine import haversine, Unit\n",
    "\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import io\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "#import pyogrio \n",
    "import time\n",
    "from time import sleep\n",
    "from progress.bar import Bar\n",
    "from numpy import interp\n",
    "\n",
    "import sys,random\n",
    "\n",
    "def progressBar(count_value, total, suffix=''):\n",
    "    bar_length = 100\n",
    "    filled_up_Length = int(round(bar_length* count_value / float(total)))\n",
    "    percentage = round(100.0 * count_value/float(total),1)\n",
    "    bar = '=' * filled_up_Length + '-' * (bar_length - filled_up_Length)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' %(bar, percentage, '%', suffix))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#    \n",
    "class Progress:\n",
    "    def __init__(self, value, end, title='Processing',buffer=50):\n",
    "        self.title = title\n",
    "        #when calling in a for loop it doesn't include the last number\n",
    "        self.end = end -1\n",
    "        self.buffer = buffer\n",
    "        self.value = value\n",
    "        self.progress()\n",
    "\n",
    "    def progress(self):\n",
    "        maped = int(interp(self.value, [0, self.end], [0, self.buffer]))\n",
    "        #print(Fore.BLUE + '======> game-ply # ',count_game,plycount,' over ',plymax)     \n",
    "        print(Fore.BLUE + f'{self.title}: [{\"#\"*maped}{\"-\"*(self.buffer - maped)}]{self.value}/{self.end} {((self.value/self.end)*100):.2f}%', end='\\r')\n",
    "\n",
    "    \n",
    "def gini(x):\n",
    "    # Mean absolute difference\n",
    "    mad = np.abs(np.subtract.outer(x, x)).mean()\n",
    "    # Relative mean absolute difference\n",
    "    rmad = mad/np.mean(x)\n",
    "    # Gini coefficient\n",
    "    g = 0.5 * rmad\n",
    "    return g\n",
    "\n",
    "def ginifast(x):\n",
    "    # The rest of the code requires numpy arrays.\n",
    "    #x = np.asarray(x)       \n",
    "    sorted_x = np.sort(x)\n",
    "    n = len(x)\n",
    "    cumx = np.cumsum(sorted_x, dtype=float)\n",
    "    # The above formula, with all weights equal to 1 simplifies to:\n",
    "    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
    "    \n",
    "    \n",
    "pd.set_option('display.max_rows', 100)\n",
    "#ServerApp.iopub_data_rate_limit\n",
    "\n",
    "%matplotlib inline\n",
    "#ox.config(log_console=True)\n",
    "ox.__version__\n",
    "#nx.config(log_console=True)\n",
    "nx.__version__\n",
    "\n",
    "### init code\n",
    "path = './dataverse_files/hong_kong-3d_gpkg.zip'\n",
    "mylist = []\n",
    "count = 0\n",
    "    \n",
    "\n",
    "#elev_all = []\n",
    "#df_elev = pd.DataFrame()\n",
    "#df_all = pd.DataFrame(columns=('City', 'Country', 'Gini', 'Min', 'Max','Mean','std','rel_std'))\n",
    "#icount = 0\n",
    "\n",
    "def func_elev(u):\n",
    "    return z[u]\n",
    "                \n",
    "def func_trans(u):\n",
    "    if u > 0:\n",
    "        return u\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "                \n",
    "### start reading \n",
    "for filename in os.listdir(path):\n",
    "    count += 1\n",
    "    country = filename.split('-')[0].capitalize()\n",
    "    mylist.append(filename)\n",
    "    \n",
    "    namefile = str(path+filename)\n",
    "    print('---- namefile= ',namefile)\n",
    "    \n",
    "    list_elev = []\n",
    "    list_ratl = []\n",
    "    list_dd = []\n",
    "    list_dz = []\n",
    "    list_dz0 = []\n",
    "    list_ratz = []\n",
    "    list_ratz_flat = []\n",
    "    list_ratbase = []\n",
    "    \n",
    "    with zipfile.ZipFile(namefile) as z:\n",
    "        for filename2 in z.namelist():\n",
    "            city = filename2.split('-')[0].capitalize()\n",
    "            \n",
    "            #filepath \n",
    "            fp =  \"zip://\"+str(namefile)+\"!\"+str(filename2)            \n",
    "            print('fp layers= ',fiona.listlayers(fp))\n",
    "            \n",
    "            # read gpd\n",
    "            gdf_nodes = gpd.read_file(fp, layer='nodes',encoding= 'unicode_escape').set_index('osmid')\n",
    "            gdf_edges = gpd.read_file(fp, layer='edges',encoding= 'unicode_escape') #.set_index(['u', 'v', 'key'])\n",
    "            assert gdf_nodes.index.is_unique and gdf_edges.index.is_unique\n",
    "            \n",
    "            z = gdf_nodes.to_dict(index=True)['elevation']\n",
    "            x = gdf_nodes.to_dict(index=True)['x']\n",
    "            y = gdf_nodes.to_dict(index=True)['y']\n",
    "            \n",
    "            dict_pos = {x[0]: (x[2],x[1]) for x in gdf_nodes.itertuples(index=True)}\n",
    "            list_elev = gdf_nodes['elevation'].to_numpy()\n",
    "            elevation = gdf_nodes['elevation'].to_numpy()\n",
    "            gini_elev = ginifast(elevation)\n",
    "            min_elev = np.min(elevation)\n",
    "            max_elev = np.max(elevation)\n",
    "            mean_elev = np.mean(elevation)\n",
    "            std_elev = np.std(elevation)\n",
    "            rel_std = std_elev/mean_elev\n",
    "            print(city,country,'gini stat= ',gini_elev,min_elev,max_elev,mean_elev,std_elev,rel_std)\n",
    "       \n",
    "            index_nodes = list(gdf_nodes.index.values)\n",
    "            \n",
    "            G3d = nx.from_pandas_edgelist(gdf_edges, 'u', 'v',\n",
    "                                    create_using=nx.Graph(), edge_attr='length_3d')\n",
    "            \n",
    "            G3d.remove_edges_from(nx.selfloop_edges(G3d))\n",
    "\n",
    "\n",
    "            print('G3d node prop= ',G3d.number_of_nodes(),G3d.size())\n",
    "            print('number of cc= ',nx.number_connected_components(G3d))\n",
    "            largest_cc = max(nx.connected_components(G3d), key=len)\n",
    "            lcc = len(largest_cc)\n",
    "            print('size of largest ',lcc)\n",
    "                 \n",
    "            # BC computation node\n",
    "            myk = lcc\n",
    "            #myk = 2000\n",
    "            print('compute BC node ',myk)\n",
    "            start_time = time.time()\n",
    "            bc_node = nx.betweenness_centrality(G3d, k=myk, weight=\"length_2d\")\n",
    "            ###########  for taking the elevation into account use:\n",
    "            #bc_node = nx.betweenness_centrality(G3d, k=myk, weight=\"length_3d\")\n",
    "            print('time= ',time.time()-start_time)\n",
    "            centrality_node = np.fromiter(bc_node.values(), float)\n",
    "            max_node, max_bc = max(bc_node.items(), key=lambda x: x[1])\n",
    "            min_node, min_bc = min(bc_node.items(), key=lambda x: x[1])\n",
    "            print('min= ',min_node, min_bc)\n",
    "            print('max= ',max_node, max_bc)\n",
    "            print('len= ',len(bc_node))\n",
    "            #print('bc node = ',bc_node)\n",
    "            #print('nodes= ',sorted(G.node\n",
    "    \n",
    "            # print map\n",
    "            fig, ax = plt.subplots(1, 1,figsize=(15,10)) \n",
    "        \n",
    "            nx.draw(G3d, pos=dict_pos, edge_color='grey', ax=ax,node_color=centrality_node*100, width=1.0, node_size=centrality_node*5e3,alpha=0.3)\n",
    "\n",
    "            fig.savefig('fig_new_BC_map_hk.pdf')\n",
    "    \n",
    "            ### 2nd figure\n",
    "            fig2, ax2 = plt.subplots(1, 1,figsize=(15,10)) \n",
    "            sample_size = myk\n",
    "            list1 = np.random.choice(index_nodes, size=sample_size)\n",
    "            sample_frac = (float(sample_size)/float(lcc))**2\n",
    "            print('relative size of sample= ',100.0*sample_frac,'%')\n",
    "\n",
    "            list_elev = []\n",
    "            list_bc = []\n",
    "            for source in list1:\n",
    "                list_elev.append(z[source])\n",
    "                list_bc.append(bc_node[source])\n",
    "                \n",
    "            x_arr = np.array(list_elev)\n",
    "            y_arr = np.array(list_bc)\n",
    "            sns.scatterplot(x=x_arr, y=y_arr, ax=ax2, color='blue',alpha=0.5,s=100.0)#,legend=False)\n",
    "            #sns.scatterplot(x=df_ard['dist'], y=df_ard['gain'], data=df_ard, ax=ax[2], color='green',alpha=0.6,s=100.0)\n",
    "            ax2.set_xlabel(r'$z$',fontsize=24)\n",
    "            ax2.set_ylabel(r'$BC$', fontsize=24)\n",
    "            ax2.tick_params(axis='both', labelsize=20) \n",
    "            \n",
    "            \n",
    "            ### print average line\n",
    "            df_ard = pd.DataFrame(columns=('z', 'bc'))\n",
    "            df_ard['z'] = x_arr\n",
    "            df_ard['bc'] = y_arr\n",
    "            df_ardnew = df_ard.loc[df_ard['z'] > 0]      \n",
    "            \n",
    "            min_z = df_ard.min(axis=0)['z']\n",
    "            max_z = df_ard.max(axis=0)['z']\n",
    "            bins = np.linspace(min_z,max_z,30)\n",
    "            group = df_ard.groupby(pd.cut(df_ard.z, bins))\n",
    "            plot_centers = (bins[:-1]+bins[1:])/2\n",
    "            plot_values = group.bc.mean()\n",
    "            ax2.plot(plot_centers,plot_values, color='lime',alpha=1.0,linewidth=3)\n",
    "            # without zeroes\n",
    "            min_z2 = df_ardnew.min(axis=0)['z']\n",
    "            max_z2 = df_ardnew.max(axis=0)['z']\n",
    "            bins2 = np.linspace(min_z2,max_z2,30)\n",
    "            group2 = df_ardnew.groupby(pd.cut(df_ardnew.z, bins2))\n",
    "            #print('group= ',df_group)\n",
    "            plot_centers2 = (bins2[:-1]+bins2[1:])/2\n",
    "            plot_values2 = group2.bc.mean()\n",
    "            ax2.plot(plot_centers2,plot_values2, color='crimson',alpha=1.0,linewidth=3)\n",
    "            \n",
    "            ax2.set(xscale='linear')\n",
    "            ax2.set(yscale='linear')\n",
    "        \n",
    "            fig2.savefig('fig_BC_vs_z_sf.pdf')\n",
    "        \n",
    "\n",
    "print('number of files= ',count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
